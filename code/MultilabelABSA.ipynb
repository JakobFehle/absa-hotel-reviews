{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd718ce6-bd1d-40c3-8ec2-a8672b00ef01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/jupyter/.local/lib/python3.8/site-packages (2.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (2.30.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (0.14.1)\n",
      "Requirement already satisfied: packaging in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jupyter/.local/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jupyter/.local/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/jupyter/.local/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jupyter/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jupyter/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jupyter/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyter/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jupyter/.local/lib/python3.8/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jupyter/.local/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/jupyter/.local/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jupyter/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jupyter/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jupyter/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /home/jupyter/.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jupyter/.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/jupyter/.local/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting optuna\n",
      "  Downloading optuna-3.2.0-py3-none-any.whl (390 kB)\n",
      "\u001b[K     |████████████████████████████████| 390 kB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting alembic>=1.5.0\n",
      "  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n",
      "\u001b[K     |████████████████████████████████| 224 kB 48.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cmaes>=0.9.1\n",
      "  Downloading cmaes-0.10.0-py3-none-any.whl (29 kB)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: numpy in /home/jupyter/.local/lib/python3.8/site-packages (from optuna) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jupyter/.local/lib/python3.8/site-packages (from optuna) (23.1)\n",
      "Collecting sqlalchemy>=1.3.0\n",
      "  Downloading SQLAlchemy-2.0.19-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 46.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/jupyter/.local/lib/python3.8/site-packages (from optuna) (4.65.0)\n",
      "Requirement already satisfied: PyYAML in /home/jupyter/.local/lib/python3.8/site-packages (from optuna) (6.0)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 15.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /home/jupyter/.local/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.9\" in /home/jupyter/.local/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (6.6.0)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /home/jupyter/.local/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (5.12.0)\n",
      "Collecting greenlet!=0.4.17; platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\")))))\n",
      "  Downloading greenlet-2.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (618 kB)\n",
      "\u001b[K     |████████████████████████████████| 618 kB 44.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /home/jupyter/.local/lib/python3.8/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jupyter/.local/lib/python3.8/site-packages (from importlib-metadata; python_version < \"3.9\"->alembic>=1.5.0->optuna) (3.15.0)\n",
      "Installing collected packages: greenlet, sqlalchemy, Mako, alembic, cmaes, colorlog, optuna\n",
      "Successfully installed Mako-1.2.4 alembic-1.11.1 cmaes-0.10.0 colorlog-6.7.0 greenlet-2.0.2 optuna-3.2.0 sqlalchemy-2.0.19\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3954eff4-3c42-4a20-9408-f5afa76de77f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from scipy.special import expit\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "import optuna\n",
    "import concurrent.futures\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, hamming_loss, precision_score, recall_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.optimization\")\n",
    "\n",
    "class CustomDataset(TorchDataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item[\"label\"] = self.labels[idx].clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "class MultiLabelABSA:\n",
    "    def __init__(self, data_path, result_path, model_name):\n",
    "        self.data_path = data_path\n",
    "        self.result_path = result_path\n",
    "        self.model_id = model_name\n",
    "    \n",
    "    def preprocess_data(self, data, tokenizer):\n",
    "        texts = data[\"text\"].tolist()\n",
    "        labels = data.iloc[:, 1:].astype(float).values.tolist()\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        encodings = tokenizer(texts, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "        return CustomDataset(encodings, labels)\n",
    "\n",
    "    def create_model(self):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_id,\n",
    "            num_labels=self.data.shape[1] - 1,\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def compute_metrics(self, eval_pred):\n",
    "        predictions, lab = eval_pred\n",
    "\n",
    "        predictions = (expit(predictions) > 0.5)\n",
    "        labels = [l==1 for l in lab]\n",
    "\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "        f1_macro = f1_score(labels, predictions, average=\"macro\", zero_division=0)\n",
    "        f1_micro = f1_score(labels, predictions, average=\"micro\", zero_division=0)\n",
    "        f1_weighted = f1_score(labels, predictions, average=\"weighted\", zero_division=0)\n",
    "\n",
    "        class_f1_scores = f1_score(labels, predictions, average=None, zero_division=0)\n",
    "\n",
    "        hamming = hamming_loss(labels, predictions)\n",
    "\n",
    "        return {\n",
    "            \"hamming_loss\": hamming,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1_macro\": f1_macro,\n",
    "            \"f1_micro\": f1_micro,\n",
    "            \"f1_weighted\": f1_weighted,\n",
    "            \"class_f1_scores\": class_f1_scores.tolist(),\n",
    "        }\n",
    "\n",
    "    def multilabel_stratified_sampling(self, data, n_splits=4, random_state=42):\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "        # Convert multilabel targets to multiclass targets\n",
    "        targets = data.iloc[:, 1:].values.argmax(axis=1)\n",
    "\n",
    "        for train_index, test_index in skf.split(data[\"text\"], targets):\n",
    "            train_data, test_data = data.iloc[train_index], data.iloc[test_index]\n",
    "            yield train_data, test_data\n",
    "\n",
    "    def objective(self, trial):        \n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", self.hyperparameters[\"learning_rate\"][0], self.hyperparameters[\"learning_rate\"][1], log=True)\n",
    "        num_train_epochs = trial.suggest_int(\"num_train_epochs\", self.hyperparameters[\"epochs\"][0], self.hyperparameters[\"epochs\"][1])\n",
    "        per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", self.hyperparameters[\"batch_size\"]) # Times two since the system uses 2 GPUs\n",
    "\n",
    "        f1_micro_scores = []\n",
    "        f1_macro_scores = []\n",
    "        f1_weighted_scores = []\n",
    "        accuracy_scores = []\n",
    "        class_f1_scores = []\n",
    "        loss = []\n",
    "        hamming = []\n",
    "\n",
    "        # Start measuring the runtime\n",
    "        start_time = time.time()\n",
    "\n",
    "        for train_data, test_data in self.multilabel_stratified_sampling(self.data, n_splits = 4, random_state = 2):\n",
    "            train_dataset = self.preprocess_data(train_data, self.tokenizer)\n",
    "            test_dataset = self.preprocess_data(test_data, self.tokenizer)\n",
    "\n",
    "            model = self.create_model()\n",
    "\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=\"output\",\n",
    "                learning_rate=learning_rate,\n",
    "                num_train_epochs=num_train_epochs,\n",
    "                per_device_train_batch_size=per_device_train_batch_size,\n",
    "                per_device_eval_batch_size=16,\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                save_strategy=\"epoch\",\n",
    "                logging_dir=\"logs\",\n",
    "                logging_steps=100,\n",
    "                logging_strategy=\"epoch\",\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"f1_micro\",\n",
    "                fp16=True,\n",
    "                report_to=\"none\"\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=test_dataset,\n",
    "                data_collator=self.data_collator,\n",
    "                tokenizer=self.tokenizer,\n",
    "                compute_metrics=self.compute_metrics\n",
    "            )\n",
    "\n",
    "            print(\"Using the following hyperparameters: lr=\" + str(learning_rate) + \" - epochs=\" + str(num_train_epochs) + \" - batch=\" + str(per_device_train_batch_size))\n",
    "\n",
    "            trainer.train()\n",
    "            eval_metrics = trainer.evaluate()\n",
    "\n",
    "            f1_micro_scores.append(eval_metrics[\"eval_f1_micro\"])\n",
    "            f1_macro_scores.append(eval_metrics[\"eval_f1_macro\"])\n",
    "            f1_weighted_scores.append(eval_metrics[\"eval_f1_weighted\"])\n",
    "            accuracy_scores.append(eval_metrics[\"eval_accuracy\"])\n",
    "            class_f1_scores.append(eval_metrics[\"eval_class_f1_scores\"])\n",
    "            loss.append(eval_metrics[\"eval_loss\"])\n",
    "            hamming.append(eval_metrics[\"eval_hamming_loss\"])\n",
    "\n",
    "\n",
    "        # Calculate runtime\n",
    "        runtime = time.time() - start_time\n",
    "\n",
    "        # Store the results in the DataFrame\n",
    "        self.results_df.loc[len(self.results_df)] = [\n",
    "            trial.number,\n",
    "            learning_rate,\n",
    "            num_train_epochs,\n",
    "            per_device_train_batch_size,\n",
    "            runtime,\n",
    "            np.mean(loss),\n",
    "            np.mean(hamming),\n",
    "            np.mean(accuracy_scores),\n",
    "            np.mean(f1_micro_scores),\n",
    "            np.mean(f1_macro_scores),\n",
    "            np.mean(f1_weighted_scores),\n",
    "            [sum(col) / len(col) for col in zip(*class_f1_scores)],\n",
    "        ]\n",
    "\n",
    "        # Save the results as a TSV file\n",
    "        self.results_df.to_csv(self.result_path, sep=\"\\t\",index=False)\n",
    "\n",
    "        return np.mean(hamming)\n",
    "            \n",
    "    def hyperparameterSearch(self, hp_config):\n",
    "        # Load data\n",
    "        self.hyperparameters = hp_config\n",
    "        \n",
    "        self.data = pd.read_csv(self.data_path, delimiter=\"\\t\", index_col=0).reset_index(drop=True)\n",
    "        self.data.columns = [\"text\"] + [f\"aspect_{i}\" for i in range(1, self.data.shape[1])]\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "\n",
    "        # Update the results_df DataFrame\n",
    "        self.results_df = pd.DataFrame(columns=[\"trial\", \"learning_rate\", \"num_train_epochs\", \"per_device_train_batch_size\", \"runtime\", \n",
    "                                           \"loss\", \"hamming_loss\", \"accuracy\", \"f1_micro\", \"f1_macro\", \"f1_weighted\", \"class_f1_scores\"])\n",
    "\n",
    "        # Optuna optimization\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(self.objective, n_trials=self.hyperparameters['num_trials'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b340c-4c36-4f14-b321-35118298bb8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_path = \"optuna_50_gbert_cat_att_pol.tsv\"\n",
    "data_path = \"./data/complete_re_df_cat_att_pol.tsv\"\n",
    "model_name = \"deepset/gbert-large\"\n",
    "\n",
    "hyperparameters = {\n",
    "    \"num_trials\": 50,\n",
    "    \"epochs\": [2,5],\n",
    "    \"batch_size\": [4,8,16], # Times the number of GPUs\n",
    "    \"learning_rate\": [1e-5, 9e-5]\n",
    "}\n",
    "\n",
    "# deepset/gbert-large\n",
    "# dbmdz/bert-base-german-uncased\n",
    "# distilbert-base-german-cased \n",
    "\n",
    "absa = MultiLabelABSA(data_path, result_path, model_name)\n",
    "absa.hyperparameterSearch(hyperparameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
