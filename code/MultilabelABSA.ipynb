{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd718ce6-bd1d-40c3-8ec2-a8672b00ef01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3954eff4-3c42-4a20-9408-f5afa76de77f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from scipy.special import expit\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "import optuna\n",
    "import concurrent.futures\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, hamming_loss, precision_score, recall_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.optimization\")\n",
    "\n",
    "class CustomDataset(TorchDataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item[\"label\"] = self.labels[idx].clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "class MultiLabelABSA:\n",
    "    def __init__(self, data_path, result_path, model_name):\n",
    "        self.data_path = data_path\n",
    "        self.result_path = result_path\n",
    "        self.model_id = model_name\n",
    "    \n",
    "    def preprocess_data(self, data, tokenizer):\n",
    "        texts = data[\"text\"].tolist()\n",
    "        labels = data.iloc[:, 1:].astype(float).values.tolist()\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        encodings = tokenizer(texts, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "        return CustomDataset(encodings, labels)\n",
    "\n",
    "    def create_model(self):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_id,\n",
    "            num_labels=self.data.shape[1] - 1,\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def compute_metrics(self, eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "\n",
    "        predictions = (expit(predictions) > 0.5).astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "        f1_macro = f1_score(labels, predictions, average=\"macro\", zero_division=0)\n",
    "        f1_micro = f1_score(labels, predictions, average=\"micro\", zero_division=0)\n",
    "        f1_weighted = f1_score(labels, predictions, average=\"weighted\", zero_division=0)\n",
    "\n",
    "        class_f1_scores = f1_score(labels, predictions, average=None, zero_division=0)\n",
    "\n",
    "        hamming = hamming_loss(labels, predictions)\n",
    "\n",
    "        return {\n",
    "            \"hamming_loss\": hamming,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1_macro\": f1_macro,\n",
    "            \"f1_micro\": f1_micro,\n",
    "            \"f1_weighted\": f1_weighted,\n",
    "            \"class_f1_scores\": class_f1_scores.tolist(),\n",
    "        }\n",
    "\n",
    "    def multilabel_stratified_sampling(self, data, n_splits=4, random_state=42):\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "        # Convert multilabel targets to multiclass targets\n",
    "        targets = data.iloc[:, 1:].values.argmax(axis=1)\n",
    "\n",
    "        for train_index, test_index in skf.split(data[\"text\"], targets):\n",
    "            train_data, test_data = data.iloc[train_index], data.iloc[test_index]\n",
    "            yield train_data, test_data\n",
    "\n",
    "    def objective(self, trial):        \n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", self.hyperparameters[\"learning_rate\"][0], self.hyperparameters[\"learning_rate\"][1], log=True)\n",
    "        num_train_epochs = trial.suggest_int(\"num_train_epochs\", self.hyperparameters[\"epochs\"][0], self.hyperparameters[\"epochs\"][1])\n",
    "        per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", self.hyperparameters[\"batch_size\"]) # Times two since the system uses 2 GPUs\n",
    "\n",
    "        f1_micro_scores = []\n",
    "        f1_macro_scores = []\n",
    "        f1_weighted_scores = []\n",
    "        accuracy_scores = []\n",
    "        class_f1_scores = []\n",
    "        loss = []\n",
    "        hamming = []\n",
    "\n",
    "        # Start measuring the runtime\n",
    "        start_time = time.time()\n",
    "\n",
    "        for train_data, test_data in self.multilabel_stratified_sampling(self.data, n_splits = 4, random_state = 2):\n",
    "            train_dataset = self.preprocess_data(train_data, self.tokenizer)\n",
    "            test_dataset = self.preprocess_data(test_data, self.tokenizer)\n",
    "\n",
    "            model = self.create_model()\n",
    "\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=\"output\",\n",
    "                learning_rate=learning_rate,\n",
    "                num_train_epochs=num_train_epochs,\n",
    "                per_device_train_batch_size=per_device_train_batch_size,\n",
    "                per_device_eval_batch_size=16,\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                save_strategy=\"epoch\",\n",
    "                logging_dir=\"logs\",\n",
    "                logging_steps=100,\n",
    "                logging_strategy=\"epoch\",\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"f1_micro\",\n",
    "                fp16=True,\n",
    "                report_to=\"none\"\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=test_dataset,\n",
    "                data_collator=self.data_collator,\n",
    "                tokenizer=self.tokenizer,\n",
    "                compute_metrics=self.compute_metrics\n",
    "            )\n",
    "\n",
    "            print(\"Using the following hyperparameters: lr=\" + str(learning_rate) + \" - epochs=\" + str(num_train_epochs) + \" - batch=\" + str(per_device_train_batch_size))\n",
    "\n",
    "            trainer.train()\n",
    "            eval_metrics = trainer.evaluate()\n",
    "\n",
    "            f1_micro_scores.append(eval_metrics[\"eval_f1_micro\"])\n",
    "            f1_macro_scores.append(eval_metrics[\"eval_f1_macro\"])\n",
    "            f1_weighted_scores.append(eval_metrics[\"eval_f1_weighted\"])\n",
    "            accuracy_scores.append(eval_metrics[\"eval_accuracy\"])\n",
    "            class_f1_scores.append(eval_metrics[\"eval_class_f1_scores\"])\n",
    "            loss.append(eval_metrics[\"eval_loss\"])\n",
    "            hamming.append(eval_metrics[\"eval_hamming_loss\"])\n",
    "\n",
    "\n",
    "        # Calculate runtime\n",
    "        runtime = time.time() - start_time\n",
    "\n",
    "        # Store the results in the DataFrame\n",
    "        self.results_df.loc[len(self.results_df)] = [\n",
    "            trial.number,\n",
    "            learning_rate,\n",
    "            num_train_epochs,\n",
    "            per_device_train_batch_size,\n",
    "            runtime,\n",
    "            np.mean(loss),\n",
    "            np.mean(hamming),\n",
    "            np.mean(accuracy_scores),\n",
    "            np.mean(f1_micro_scores),\n",
    "            np.mean(f1_macro_scores),\n",
    "            np.mean(f1_weighted_scores),\n",
    "            [sum(col) / len(col) for col in zip(*class_f1_scores)],\n",
    "        ]\n",
    "\n",
    "        # Save the results as a TSV file\n",
    "        self.results_df.to_csv(self.result_path, sep=\"\\t\",index=False)\n",
    "\n",
    "        return np.mean(hamming)\n",
    "            \n",
    "    def hyperparameterSearch(self, hp_config):\n",
    "        # Load data\n",
    "        self.hyperparameters = hp_config\n",
    "        \n",
    "        self.data = pd.read_csv(self.data_path, delimiter=\"\\t\", index_col=0).reset_index(drop=True)\n",
    "        self.data.columns = [\"text\"] + [f\"aspect_{i}\" for i in range(1, self.data.shape[1])]\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "\n",
    "        # Update the results_df DataFrame\n",
    "        self.results_df = pd.DataFrame(columns=[\"trial\", \"learning_rate\", \"num_train_epochs\", \"per_device_train_batch_size\", \"runtime\", \n",
    "                                           \"loss\", \"hamming_loss\", \"accuracy\", \"f1_micro\", \"f1_macro\", \"f1_weighted\", \"class_f1_scores\"])\n",
    "\n",
    "        # Optuna optimization\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(self.objective, n_trials=self.hyperparameters['num_trials'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b340c-4c36-4f14-b321-35118298bb8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_path = \"optuna_50_gbert_cat_att_pol.tsv\"\n",
    "data_path = \"./data/complete_re_df_cat_att_pol.tsv\"\n",
    "model_name = \"deepset/gbert-large\"\n",
    "\n",
    "hyperparameters = {\n",
    "    \"num_trials\": 50,\n",
    "    \"epochs\": [2,5],\n",
    "    \"batch_size\": [4,8,16], # Times the number of GPUs\n",
    "    \"learning_rate\": [1e-5, 9e-5]\n",
    "}\n",
    "\n",
    "# deepset/gbert-large\n",
    "# dbmdz/bert-base-german-uncased\n",
    "# distilbert-base-german-cased \n",
    "\n",
    "absa = MultiLabelABSA(data_path, result_path, model_name)\n",
    "absa.hyperparameterSearch(hyperparameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
